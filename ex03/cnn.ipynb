{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "## Introduction\n",
    "We install PyTorch using the command `conda install pytorch`. Then we run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d\n",
    "\n",
    "\n",
    "\n",
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "test_dataset = dset.MNIST(\"./\", download=True,\n",
    "                          train=False,\n",
    "                          transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=mb_size,\n",
    "                                          shuffle=True, num_workers=1,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1-alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o):\n",
    "    h = rectify(X @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 0.40162765979766846\n",
      "Average Test Loss: 0.2319663017988205\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 0.16438473761081696\n",
      "Average Test Loss: 0.3202357888221741\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 0.09217559546232224\n",
      "Average Test Loss: 0.3900783061981201\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 0.0667547807097435\n",
      "Average Test Loss: 0.5949299931526184\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 0.049366917461156845\n",
      "Average Test Loss: 0.7628595232963562\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 0.03868158161640167\n",
      "Average Test Loss: 0.7551425695419312\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 0.03402946516871452\n",
      "Average Test Loss: 0.8039210438728333\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 0.02767530456185341\n",
      "Average Test Loss: 0.813590943813324\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 0.014047685079276562\n",
      "Average Test Loss: 0.9228616952896118\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 0.014962374232709408\n",
      "Average Test Loss: 0.8535373210906982\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 0.011429532431066036\n",
      "Average Test Loss: 0.797049343585968\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "        noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "In the following cell we implement the dropout function and a dropout model. Dropout helps the network to not \"overestimate\" the meaning of one single input: Because each channel will drop out at some time, the tends to learn information that do not depend on only one special input or combination of inputs. The training set can contain \"random\" similarities that have nothing to do with a property of the distribution we draw our instances from. Dropout helps to suppress the influence of these similarities. For evaluating the test loss, we do not apply dropout anymore (this would mean that we throw away information unnecessarily), but use the same model configuration without dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, p_drop = .5):\n",
    "    \"\"\"\n",
    "    Implements dropout.\n",
    "    \n",
    "    Sets entries with a probability of p_drop to zero.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : A Tensor\n",
    "    \n",
    "    p_drop : dropout probability\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Tensor\n",
    "        X after dropout.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if (p_drop > 0 and p_drop < 1):\n",
    "        active = torch.tensor(np.random.binomial(1,1-p_drop,size=np.array(X.shape)))\n",
    "        X *= active / (1-p_drop)\n",
    "        return X\n",
    "        \n",
    "    else: \n",
    "        return X\n",
    "    \n",
    "    \n",
    "\n",
    "def dropout_model(X, w_h, w_h2, w_o, p_drop_input=0.5, p_drop_hidden=0.5):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 1.0189917087554932\n",
      "Average Test Loss: 0.32450130581855774\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 1.0233594179153442\n",
      "Average Test Loss: 0.3054808974266052\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 1.2779194116592407\n",
      "Average Test Loss: 0.36670178174972534\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 1.5280224084854126\n",
      "Average Test Loss: 0.45627361536026\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 1.7229119539260864\n",
      "Average Test Loss: 0.5553186535835266\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 1.8203853368759155\n",
      "Average Test Loss: 0.656729519367218\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 1.9705488681793213\n",
      "Average Test Loss: 0.5806933045387268\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 1.9998868703842163\n",
      "Average Test Loss: 0.7037334442138672\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 2.1543989181518555\n",
      "Average Test Loss: 0.6577818393707275\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 2.1684606075286865\n",
      "Average Test Loss: 0.7350997924804688\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 2.2698614597320557\n",
      "Average Test Loss: 0.7763116359710693\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "        noise_py_x = dropout_model(X.reshape(mb_size, 784), w_h, w_h2, w_o)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the test error from the dropout model and the model without dropout. In case of the dropout model our test loss starts (after epoch 1) with a higher value than the model without dropout. But the test loss increases much slower when you use the dropout model (avg test loss of 0.78 vs. 0.80 after 101 epochs).\n",
    "## Parametric ReLU\n",
    "In the next cell, we define the parametric ReLU activation function and add the parameters a into the params list. We define a PRelu model that uses the PRelu activation instead of Relu and uses dropout. For evaluating the test loss another Prelu model without dropout is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "    return torch.max(torch.zeros_like(X), X) - torch.max(torch.zeros_like(X), -X) * a\n",
    "\n",
    "def dropout_PRelu_model(X, w_h, w_h2, w_o, a_h, a_h2, p_drop_input=0.5, p_drop_hidden=0.5):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h, a_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2, a_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "def PRelu_model(X, w_h, w_h2, w_o, a_h, a_h2):\n",
    "    h = PRelu(X @ w_h, a_h)\n",
    "    h2 = PRelu(h @ w_h2, a_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average Train Loss: 1.0492351055145264\n",
      "Average Test Loss: 0.3687511384487152\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Average Train Loss: 0.516887366771698\n",
      "Average Test Loss: 0.1784283071756363\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Average Train Loss: 0.5752396583557129\n",
      "Average Test Loss: 0.18248169124126434\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Average Train Loss: 0.5795792937278748\n",
      "Average Test Loss: 0.18883413076400757\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Average Train Loss: 0.5838701725006104\n",
      "Average Test Loss: 0.15499158203601837\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Average Train Loss: 0.5781242251396179\n",
      "Average Test Loss: 0.1776753067970276\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Average Train Loss: 0.5938964486122131\n",
      "Average Test Loss: 0.16523495316505432\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Average Train Loss: 0.6122684478759766\n",
      "Average Test Loss: 0.1741521954536438\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Average Train Loss: 0.6047658920288086\n",
      "Average Test Loss: 0.1833423674106598\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Average Train Loss: 0.6154748797416687\n",
      "Average Test Loss: 0.17329677939414978\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Average Train Loss: 0.6178078651428223\n",
      "Average Test Loss: 0.17919178307056427\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "#initialize the weights for the PRelus as 0.25\n",
    "a_h = torch.ones(625) * 0.25     \n",
    "a_h.requires_grad = True\n",
    "a_h2 = torch.ones(625) * 0.25\n",
    "a_h2.requires_grad = True\n",
    "\n",
    "#add a_h and a_h2 to the params list\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a_h, a_h2])\n",
    "\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for i in range(101):\n",
    "    print(\"Epoch: {}\".format(i+1))\n",
    "    avg_train_loss = 0.\n",
    "    for (j, (X, y)) in enumerate(dataloader):\n",
    "        noise_py_x = dropout_PRelu_model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a_h, a_h2)\n",
    "        optimizer.zero_grad()\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "        avg_train_loss += cost\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Average Train Loss: {}\".format(avg_train_loss / (j + 1)))\n",
    "\n",
    "        # no need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            avg_test_loss = 0.\n",
    "            for (k, (X, y)) in enumerate(test_loader):\n",
    "                noise_py_x = PRelu_model(X.reshape(mb_size, 784), w_h, w_h2, w_o, a_h, a_h2)\n",
    "                cost = torch.nn.functional.cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                avg_test_loss += cost\n",
    "\n",
    "            print(\"Average Test Loss: {}\".format(avg_test_loss / (k + 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results to the Relu model and the dropout Relu model. Similar to the dropout Relu model the test loss is larger for the dropout PRelu model than for the Relu model without dropout. Another similarity is that the dropout Relu model and the dropout PRelu model have a test loss which is increasing slow with the number of epochs (compared to the Relu model without dropout). But we can see that the PReLU model decreases its test loss in the beginning for more than ten epochs. This is something new compared to all the models before. The other models did not show any decrease of the test error (when you display the test loss after steps of 10 epochs).\n",
    "## Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import conv2d, max_pool2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
